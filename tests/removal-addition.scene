print ##################################################
print # test that we can
print # 1. remove a master
print # 2. add a master
print ##################################################

devel enable salt=True

cluster tfvars images-devel
cluster tfvars tests/removal-addition

stage apply
print ###############################################
print # creating cluster
print ###############################################

cluster create
sleep 30

# wait for nodes to be accepted
salt wait
ctl minions accepted
sleep 30

cluster snapshot stage="post-create"
sleep 30

stage orch
print ###############################################
print # bootstrapping
print ###############################################
cluster rollback
sleep 30

# make sure the salt-minion is up and running
ssh nodes "systemctl restart salt-minion"

# check nodes->dashboard connectivity before going further
# (this will abort the script if it fails)
ssh nodes "ping -c1 dashboard"

# run the regular bootstrap orchestration
salt wait
orch boot
sleep 30

# some post-bootstrap checks
#ssh node-1 "set -a ; source /etc/sysconfig/etcdctl ; set +a ; etcdctl cluster-health"
ssh node-1 "kubectl get nodes"

#  cluster snapshot stage="post-orch", description="Orchestration has been run"
#  sleep 30

stage orch-rm
print ###############################################
print # removing master caasp-node-1
print ###############################################
#  cluster rollback
sleep 30

orch rm caasp-node-1
sleep 30

# check if the kubectl is fine
# orch kubeconfig

stage test1-tests
print "Checking nodes can get the list of nodes"
test cmd_at node-0 kubectl get nodes

print "Checking nodes see >=3 nodes alive"
test cmd_at node-0 [ \$(kubectl get nodes | grep caasp-node | grep -v NotReady | wc -l) -ge 3 ]

#  TODO: check we can see 2 masters
#  TODO: check caasp-node-1 is not there

cluster snapshot stage="post-rm", description="caasp-node-1 has been removed from the cluster"
sleep 30

stage orch-add
print ###############################################
print # adding master caasp-node-6
print ###############################################
cluster rollback
sleep 30

print "Setting node-6 as a master"
ssh run caasp-node-6 echo -e 'roles: [ kube-master ]' > /etc/salt/grains

# caasp-node-6 should be accepted, so a orch boot should setup the new node
orch boot
sleep 30

stage test2-tests
print "Checking nodes can get the list of nodes"
test cmd_at node-0 kubectl get nodes

print "Checking nodes see >=3 nodes alive"
test cmd_at node-0 [ \$(kubectl get nodes | grep caasp-node | grep -v NotReady | wc -l) -ge 3 ]

#  TODO: check we can see 3 masters
#  TODO: check caasp-node-6 is there
#  TODO: check caasp-node-1 is not there
